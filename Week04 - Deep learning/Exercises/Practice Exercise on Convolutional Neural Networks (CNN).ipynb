{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a07a343",
   "metadata": {},
   "source": [
    "# Practice Exercise on Convolutional Neural Networks (CNN)\n",
    "\n",
    "Welcome to the Practice Exercise on Convolutional Neural Networks (CNN). In this exercise, we will focus on an image classification task where the goal is to predict whether an image contains a cat or a dog. We will work with a dataset of labeled images and build, train, and evaluate a CNN model. This practice will allow you to apply your understanding of CNNs to achieve high accuracy in image classification.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "### **Dataset Name:** Cats and Dogs Image Dataset\n",
    "\n",
    "### **Description:**  \n",
    "The dataset contains images of cats and dogs labeled for classification purposes. Each image belongs to one of the two classes: 'Cat' or 'Dog'. The goal is to classify the images correctly based on the content (i.e., whether the image is of a cat or a dog). The dataset is often used to test image classification models.\n",
    "\n",
    "### **Features:**\n",
    "There are two main folders which are:\n",
    "- `Cat`: Images labeled as containing a cat.\n",
    "- `Dog`: Images labeled as containing a dog.\n",
    "\n",
    "### **Target Variable:**\n",
    "- The goal is to predict whether an image contains a cat or a dog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f5ed0",
   "metadata": {},
   "source": [
    "Dataset: https://www.kaggle.com/datasets/samuelcortinhas/cats-and-dogs-image-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb05577",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3631d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout,Rescaling\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f58bb247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 files belonging to 2 classes.\n",
      "Using 474 files for training.\n",
      "Found 557 files belonging to 2 classes.\n",
      "Using 83 files for validation.\n",
      "Found 140 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = image_dataset_from_directory(\n",
    "    'Datasets/archive/train',\n",
    "    validation_split=0.15,\n",
    "    # shuffle=True,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(64, 64)\n",
    ")\n",
    "\n",
    "val = image_dataset_from_directory(\n",
    "    'Datasets/archive/train',\n",
    "    validation_split=0.15,\n",
    "    # shuffle=True,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(64, 64)\n",
    ")\n",
    "\n",
    "test = image_dataset_from_directory(\n",
    "    'Datasets/archive/test',\n",
    "    seed=123,\n",
    "    image_size=(64, 64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5be8d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,   \n",
    "#     validation_split=0.15, \n",
    "#     )\n",
    "\n",
    "# test_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     )\n",
    "\n",
    "# train = train_datagen.flow_from_directory(\n",
    "#     'Datasets/archive/train',\n",
    "#     target_size=(64, 64),\n",
    "#     class_mode='binary',\n",
    "#     subset='training',\n",
    "#     seed=123\n",
    "# )\n",
    "\n",
    "# val = train_datagen.flow_from_directory(\n",
    "#     'Datasets/archive/train',\n",
    "#     target_size=(64, 64),\n",
    "#     class_mode='binary',\n",
    "#     subset='validation',\n",
    "#     seed=123\n",
    "# )\n",
    "\n",
    "# test = test_datagen.flow_from_directory(\n",
    "#     'Datasets/archive/test',\n",
    "#     target_size=(64, 64),\n",
    "#     class_mode='binary',\n",
    "#     subset='training',\n",
    "#     seed=123\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa968f9e",
   "metadata": {},
   "source": [
    "\n",
    "We will start by loading the dataset and preprocessing the images. This includes:\n",
    "- Resizing images .\n",
    "- Normalizing pixel values.\n",
    "\n",
    "Add more if needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2230c4b",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "In this section, we will split our dataset into three parts:\n",
    "\n",
    "* Training set (70%): This portion of the dataset is used to train the CNN model.\n",
    "* Validation set (15%): This portion is used to validate the model during training, helping us tune hyperparameters and avoid overfitting.\n",
    "* Test set (15%): This portion is used to evaluate the model after training, to check its generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ce960",
   "metadata": {},
   "source": [
    "## Building the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1a1cb",
   "metadata": {},
   "source": [
    "\n",
    "Now, we will define our CNN architecture using `tensorflow.keras`. The architecture will consist of:\n",
    "- Convolutional layers followed by max-pooling layers\n",
    "- Flatten layer\n",
    "- Dense layers\n",
    "- Output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "91a5150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/preprocessing/tf_data_layer.py:18: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Rescaling(1./255, input_shape=(64, 64, 3)))\n",
    "\n",
    "# # Hidding layer\n",
    "model.add(Conv2D(12,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "# Dropout(0.25)\n",
    "\n",
    "model.add(Conv2D(20,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "# Dropout(0.25)\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=6, activation='relu'))\n",
    "# Dropout(0.25)\n",
    "\n",
    "model.add(Dense(units=12, activation='relu'))\n",
    "# Dropout(0.25)\n",
    "\n",
    "\n",
    "# Output layer\n",
    "\n",
    "# class_labels_numbers <= 2 -> dense layer units = 1    ===== activation='sigmoid'\n",
    "# class_labels_numbers > 2 -> dense layer units = class_labels_numbers ====== activation='softmax'\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "36c980bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fff4b8",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84cb05c",
   "metadata": {},
   "source": [
    "\n",
    "Train the CNN model using the `fit` function. We will use the training and validation we created earlier.\n",
    "\n",
    "Fill in the code to train the model for a specified number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0e11ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.4978 - loss: 0.6976 - val_accuracy: 0.5060 - val_loss: 0.6948\n",
      "Epoch 2/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.4951 - loss: 0.6954 - val_accuracy: 0.4819 - val_loss: 0.6937\n",
      "Epoch 3/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.5263 - loss: 0.6922 - val_accuracy: 0.4819 - val_loss: 0.6939\n",
      "Epoch 4/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.4927 - loss: 0.6933 - val_accuracy: 0.4819 - val_loss: 0.6937\n",
      "Epoch 5/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5330 - loss: 0.6907 - val_accuracy: 0.5181 - val_loss: 0.6940\n",
      "Epoch 6/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.5405 - loss: 0.6893 - val_accuracy: 0.5060 - val_loss: 0.6934\n",
      "Epoch 7/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5329 - loss: 0.6856 - val_accuracy: 0.4940 - val_loss: 0.6924\n",
      "Epoch 8/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.5039 - loss: 0.6781 - val_accuracy: 0.4940 - val_loss: 0.6917\n",
      "Epoch 9/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.4999 - loss: 0.6772 - val_accuracy: 0.5301 - val_loss: 0.6908\n",
      "Epoch 10/10\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.5720 - loss: 0.6721 - val_accuracy: 0.4458 - val_loss: 0.6871\n"
     ]
    }
   ],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stopping = EarlyStopping(monitor='val_loss',patience=3)\n",
    "\n",
    "history = model.fit(train,epochs=10,validation_data=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba878f4",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508bce7",
   "metadata": {},
   "source": [
    "\n",
    "After training, evaluate the model on the validation data to check its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5cd3a753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4416 - loss: 0.6913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6871176958084106, 0.4457831382751465]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0c1b9",
   "metadata": {},
   "source": [
    "## Testing with New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94a8ae",
   "metadata": {},
   "source": [
    "Finally, let's test the model with some new images. Preprocess the images and use the trained model to predict whether the image is of a cat or a dog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cf379ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x30aeb2200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "0.4642857142857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 22:32:27.005378: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "prediction = model.predict(test) > 0.5\n",
    "\n",
    "print(accuracy_score(np.concatenate([y for x, y in test], axis=0),prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
